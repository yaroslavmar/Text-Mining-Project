{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import textmining as tm\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "from nltk.corpus import names\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "    \n",
    "#12/5/2000 is too short\n",
    "#out=[\"19/6/1998\", \"17/9/1998\", \"14/10/1998\", \"23/10/1998\", \"9/2/1999\", \"13/10/1999\", \"12/5/2000\", \"16/5/2001\", \"10/7/2001\",\n",
    "#    \"27/2/2003\", \"24/3/2005\", \"23/3/2006\", \"4/9/2006\", \"20/6/2008\", \"30/6/2008\", \"15/9/2008\", \"22/9/2008\", \"23/12/2008\",\n",
    "#    \"5/9/2011\"]\n",
    "#out_index = list(map((lambda x:  fecha.index(x)), out)) \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Remove speeches in other languages (German or French)\n",
    "numbers=range(1,578,1)\n",
    "#out_index=[0, 4, 9, 11, 28, 45, 56, 80, 88, 126, 197, 229, 243, 322, 324, 331, 333, 347, 461]\n",
    "out_index=[0, 4, 9, 11, 29, 45, 56, 80, 88, 126, 197, 229, 243, 322, 324, 331, 333, 347, 461, 163, 278, 443]\n",
    "out_index=[num+1 for num in out_index]\n",
    "numbers=[num for num in numbers if num not in out_index]\n",
    "  \n",
    "\n",
    "# Load data\n",
    "data=[]\n",
    "fecha=[]\n",
    "for i in numbers:\n",
    "    file_name=str(i) + \".json\"\n",
    "    path=\"../data/\" + file_name\n",
    "    \n",
    "    with open(path, \"r\") as fi:\n",
    "        new = json.load(fi)\n",
    "    \n",
    "    fecha.append(str(new[3]))\n",
    "    new=new[0]\n",
    "    data.append(new)   \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "date=fecha.join(\"\\n\")\n",
    "for item in fecha:\n",
    "    myfile.write(item)\n",
    "myfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get stopwords\n",
    "stopwords=open(\"data/stopwords.txt\").read()\n",
    "stopwords=stopwords.split(\"\\n\")\n",
    "stopwords=[x.replace ( \"\\r\", \"\" ) for x in stopwords] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Document Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create document term matrix\n",
    "tdm = tm.TermDocumentMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_data=[]\n",
    "\n",
    "for speech in data:\n",
    "    \n",
    "    # tokenize words\n",
    "    tokenize = nltk.word_tokenize(speech)\n",
    "    tokenize = [token for token in tokenize if (token.isalpha() and len(token) > 2)]\n",
    "    \n",
    "    # Check if the word is in English\n",
    "    tokenize=[word for word in tokenize if wordnet.synsets(word) ]\n",
    "    \n",
    "    # eliminate stopwords \n",
    "    stop_words = [word for word in tokenize if word not in stopwords]\n",
    "    \n",
    "    # stem\n",
    "    stemmer = [PorterStemmer().stem(t) for t in stop_words]\n",
    "    \n",
    "    # create speech as single string\n",
    "    speech2 = [str(i) for i in stemmer]\n",
    "    speech2 = \" \".join(speech2)\n",
    "    \n",
    "    # save for tf-idf\n",
    "    new_data.append(speech2)\n",
    "    \n",
    "    # add to dtm\n",
    "    tdm.add_doc(speech2)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SAVE IN CSV FILE\n",
    "tdm.write_csv('data/tdm.csv', cutoff=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
